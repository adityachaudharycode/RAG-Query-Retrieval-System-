# ğŸ  Local-Only LLM Server Guide

## ğŸ¯ **What You Have**
A complete FastAPI server that runs **entirely on your local machine** using your existing Ollama setup:
- âœ… Ollama installed
- âœ… llama3.2:3b (text generation)
- âœ… llama3.2:1b (fallback)
- âœ… nomic-embed-text (embeddings)

## ğŸš€ **Quick Start**

### **Step 1: Start Ollama Service**
```bash
ollama serve
```
Keep this running in a separate terminal.

### **Step 2: Start Local LLM Server**
```bash
python run_local_only.py
```

You'll see:
```
ğŸš€ Starting Local-Only LLM FastAPI Server
ğŸ  Local LLM with Ollama
ğŸŒ Server: http://localhost:8000
ğŸ“– API Docs: http://localhost:8000/docs
ğŸ¯ Endpoint: POST /api/v1/hackrx/run

ğŸ  Starting Local-Only LLM Server
âœ… Local LLM server ready!
ğŸŒ Server running at: http://localhost:8000
ğŸ“– API docs at: http://localhost:8000/docs
ğŸ¯ Test endpoint: POST /api/v1/hackrx/run
```

### **Step 3: Test Your Server**
```bash
# In another terminal
python test_local_server.py
```

## ğŸ¯ **API Endpoints**

### **Main Endpoint: `/api/v1/hackrx/run`**
**Method:** POST  
**Content-Type:** application/json

**Request Format:**
```json
{
  "documents": "https://example.com/document.pdf",
  "questions": [
    "What is the grace period for premium payment?",
    "What is the waiting period for cataract surgery?"
  ]
}
```

**Response Format:**
```json
{
  "answers": [
    "The grace period for premium payment is 30 days...",
    "The waiting period for cataract surgery is 2 years..."
  ]
}
```

### **Health Check: `/health`**
**Method:** GET

**Response:**
```json
{
  "status": "healthy",
  "version": "1.0.0",
  "local_llm_available": true,
  "models": {
    "embedding": "nomic-embed-text",
    "text": "llama3.2:3b",
    "fallback": "llama3.2:1b"
  }
}
```

### **Root: `/`**
**Method:** GET  
Returns system information and available endpoints.

## ğŸ§ª **Testing with cURL**

### **Health Check:**
```bash
curl http://localhost:8000/health
```

### **Main Endpoint:**
```bash
curl -X POST "http://localhost:8000/api/v1/hackrx/run" \
  -H "Content-Type: application/json" \
  -d '{
    "documents": "https://hackrx.blob.core.windows.net/assets/policy.pdf?sv=2023-01-03&st=2025-07-04T09%3A11%3A24Z&se=2027-07-05T09%3A11%3A00Z&sr=b&sp=r&sig=N4a9OU0w0QXO6AOIBiu4bpl7AXvEZogeT%2FjUHNO7HzQ%3D",
    "questions": [
      "What is the grace period for premium payment?",
      "What is the waiting period for cataract surgery?"
    ]
  }'
```

## ğŸ§ª **Testing with Postman**

### **Setup:**
1. **Method**: POST
2. **URL**: `http://localhost:8000/api/v1/hackrx/run`
3. **Headers**: 
   - `Content-Type: application/json`
4. **Body** (raw JSON):
```json
{
  "documents": "https://hackrx.blob.core.windows.net/assets/policy.pdf?sv=2023-01-03&st=2025-07-04T09%3A11%3A24Z&se=2027-07-05T09%3A11%3A00Z&sr=b&sp=r&sig=N4a9OU0w0QXO6AOIBiu4bpl7AXvEZogeT%2FjUHNO7HzQ%3D",
  "questions": [
    "What is the grace period for premium payment?",
    "What is the waiting period for cataract surgery?",
    "What is the waiting period for pre-existing diseases?",
    "Does this policy cover maternity expenses?",
    "What is the No Claim Discount offered?"
  ]
}
```

## ğŸ“Š **Expected Performance**

### **First Request:**
- **Time**: 30-60 seconds (model loading + processing)
- **Process**: Document download â†’ Text extraction â†’ Chunking â†’ Embeddings â†’ Vector store â†’ Q&A

### **Subsequent Requests:**
- **Time**: 15-30 seconds (models already loaded)
- **Process**: Same pipeline but faster due to warm models

### **Console Output:**
```
ğŸ”„ Processing request with 5 questions
ğŸ“„ Document: https://hackrx.blob.core.windows.net/assets/policy.pdf...
ğŸ“„ Processing document...
âœ… Extracted 45,231 characters from document
âœ‚ï¸  Creating chunks and building vector store...
âœ… Created 52 chunks
ğŸ” Generating embeddings for 52 texts using nomic-embed-text
âœ… Generated 52 embeddings in 15.3s
âœ… Vector store built with 52 chunks
â“ Processing 5 questions...
   Question 1/5: What is the grace period for premium payment?...
   âœ… Answer 1 generated
   Question 2/5: What is the waiting period for cataract surgery?...
   âœ… Answer 2 generated
   ...
âœ… Request completed successfully!
â±ï¸  Total time: 45.2 seconds
âš¡ Average per question: 9.0 seconds
ğŸ’° Cost: $0.00 (completely free!)
```

## ğŸ¯ **Key Features**

### **Zero API Costs:**
- No Gemini API calls
- No OpenAI API calls
- No Perplexity API calls
- **100% free operation**

### **No Rate Limits:**
- Process unlimited requests
- No daily/monthly quotas
- No throttling

### **Complete Privacy:**
- Documents processed locally
- No data sent to external APIs
- Everything stays on your machine

### **Offline Capable:**
- Works without internet (after document download)
- No dependency on external services
- Self-contained system

## ğŸ”§ **Troubleshooting**

### **Server Won't Start:**
```bash
# Check if Ollama is running
curl http://localhost:11434/api/version

# If not, start Ollama
ollama serve
```

### **Models Not Found:**
```bash
# Check available models
ollama list

# Download missing models
ollama pull nomic-embed-text
ollama pull llama3.2:3b
ollama pull llama3.2:1b
```

### **Slow Performance:**
- **First request**: Normal (model loading)
- **Subsequent requests**: Should be faster
- **Large documents**: Will take longer to process
- **Many questions**: Linear scaling

### **Memory Issues:**
- **RAM Usage**: ~4-6GB for models + processing
- **Solution**: Close other applications or use smaller model (llama3.2:1b)

## ğŸ“ˆ **Performance Tuning**

### **In `run_local_only.py`, you can adjust:**

```python
# Chunk size (larger = more context, slower)
chunk_size = 1000  # Try 500-2000

# Number of similar chunks (more = better context, slower)
top_k = 2  # Try 1-5

# Model temperature (lower = more deterministic)
temperature = 0.1  # Try 0.0-0.3

# Max tokens (longer answers)
num_predict = 500  # Try 200-1000
```

## ğŸ‰ **Benefits Summary**

| Feature | Local Server | API-based |
|---------|-------------|-----------|
| **Cost** | $0.00 | $50-100/month |
| **Speed** | 30-60s | 60-300s |
| **Rate Limits** | None | Frequent |
| **Privacy** | 100% local | Data sent to cloud |
| **Reliability** | Always available | API dependent |
| **Offline** | âœ… Works | âŒ Needs internet |

## ğŸš€ **Production Ready**

Your local server is now:
- âœ… **API compatible** with existing systems
- âœ… **FastAPI powered** with automatic docs
- âœ… **Error handling** and proper HTTP responses
- âœ… **Health monitoring** endpoints
- âœ… **CORS enabled** for web integration
- âœ… **Scalable** (can handle multiple concurrent requests)

**Your local-only LLM server is ready for production use!** ğŸ‰
