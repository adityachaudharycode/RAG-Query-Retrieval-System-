"""
Performance test script to compare optimized vs original performance
"""
import asyncio
import time
import requests
import json
from datetime import datetime


# Test configuration
API_BASE_URL = "http://localhost:8000"
BEARER_TOKEN = "87e1f87355284c19bac1880413d4a1e7cb868891939fc1c6d8227ee2c1b89cb0"  # From .env.example

# Sample test data
TEST_DOCUMENT_URL = "https://hackrx.blob.core.windows.net/assets/policy.pdf?sv=2023-01-03&st=2025-07-04T09%3A11%3A24Z&se=2027-07-05T09%3A11%3A00Z&sr=b&sp=r&sig=N4a9OU0w0QXO6AOIBiu4bpl7AXvEZogeT%2FjUHNO7HzQ%3D"

TEST_QUESTIONS = [
    "What is the grace period for premium payment?",
    "What is the waiting period for cataract surgery?",
    "What is the waiting period for pre-existing diseases?"
]


def print_performance_banner():
    """Print performance test banner"""
    print("=" * 70)
    print("‚ö° PERFORMANCE TEST - LLM Query-Retrieval System")
    print("=" * 70)
    print(f"üïí Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìÑ Document: Policy PDF")
    print(f"‚ùì Questions: {len(TEST_QUESTIONS)}")
    print("=" * 70)


def test_api_request():
    """Test the optimized API performance"""
    print("üöÄ Testing optimized API performance...")
    
    # Prepare request
    headers = {
        "Authorization": f"Bearer {BEARER_TOKEN}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "documents": TEST_DOCUMENT_URL,
        "questions": TEST_QUESTIONS
    }
    
    # Measure performance
    start_time = time.time()
    
    try:
        response = requests.post(
            f"{API_BASE_URL}/hackrx/run",
            headers=headers,
            json=payload,
            timeout=120  # 2 minute timeout
        )
        
        end_time = time.time()
        total_time = end_time - start_time
        
        if response.status_code == 200:
            result = response.json()
            answers = result.get("answers", [])
            
            print(f"‚úÖ Request completed successfully!")
            print(f"‚è±Ô∏è  Total time: {total_time:.1f} seconds")
            print(f"üìä Answers received: {len(answers)}")
            print(f"‚ö° Average time per question: {total_time/len(TEST_QUESTIONS):.1f} seconds")
            
            # Performance assessment
            if total_time <= 30:
                print("üéØ EXCELLENT: Under 30 seconds!")
            elif total_time <= 60:
                print("‚úÖ GOOD: Under 1 minute")
            elif total_time <= 120:
                print("‚ö†Ô∏è  ACCEPTABLE: Under 2 minutes")
            else:
                print("‚ùå SLOW: Over 2 minutes")
            
            return total_time, answers
            
        else:
            print(f"‚ùå Request failed with status: {response.status_code}")
            print(f"Error: {response.text}")
            return None, None
            
    except requests.exceptions.Timeout:
        print("‚ùå Request timed out (over 2 minutes)")
        return None, None
    except Exception as e:
        print(f"‚ùå Request failed: {str(e)}")
        return None, None


def check_server_health():
    """Check if the server is running"""
    try:
        response = requests.get(f"{API_BASE_URL}/health", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Server is healthy and ready")
            return True
        else:
            print(f"‚ö†Ô∏è  Server health check failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå Cannot connect to server: {str(e)}")
        print("üí° Make sure the server is running with: python run-fast.py")
        return False


def display_performance_summary(total_time, answers):
    """Display performance summary"""
    print("\n" + "=" * 70)
    print("üìä PERFORMANCE SUMMARY")
    print("=" * 70)
    
    if total_time and answers:
        print(f"‚è±Ô∏è  Total execution time: {total_time:.1f} seconds")
        print(f"‚ùì Questions processed: {len(answers)}")
        print(f"‚ö° Average per question: {total_time/len(answers):.1f} seconds")
        
        # Performance benchmarks
        print("\nüìà Performance Benchmarks:")
        print(f"   üéØ Target (30s):     {'‚úÖ ACHIEVED' if total_time <= 30 else '‚ùå MISSED'}")
        print(f"   ‚úÖ Good (60s):       {'‚úÖ ACHIEVED' if total_time <= 60 else '‚ùå MISSED'}")
        print(f"   ‚ö†Ô∏è  Acceptable (120s): {'‚úÖ ACHIEVED' if total_time <= 120 else '‚ùå MISSED'}")
        
        # Optimization impact
        original_time = 300  # 5 minutes (original performance)
        improvement = ((original_time - total_time) / original_time) * 100
        print(f"\nüöÄ Performance Improvement:")
        print(f"   Original time: ~{original_time} seconds")
        print(f"   Optimized time: {total_time:.1f} seconds")
        print(f"   Improvement: {improvement:.1f}% faster")
        
        # Sample answers
        print(f"\nüìù Sample Answers:")
        for i, answer in enumerate(answers[:2], 1):
            print(f"   Q{i}: {answer[:100]}...")
    else:
        print("‚ùå No performance data available")
    
    print("=" * 70)


def main():
    """Main performance test function"""
    print_performance_banner()
    
    # Check server health
    if not check_server_health():
        return
    
    print("\nüîÑ Starting performance test...")
    
    # Run the performance test
    total_time, answers = test_api_request()
    
    # Display results
    display_performance_summary(total_time, answers)
    
    print(f"\nüïí Test completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


if __name__ == "__main__":
    main()
