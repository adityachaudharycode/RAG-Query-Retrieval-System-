"""
Test script for local-only LLM server
Tests the /api/v1/hackrx/run endpoint
"""
import requests
import json
import time
from datetime import datetime


def test_local_server():
    """Test the local LLM server"""
    print("ğŸ§ª TESTING LOCAL-ONLY LLM SERVER")
    print("=" * 60)
    print(f"ğŸ•’ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Server configuration
    BASE_URL = "http://localhost:8000"
    
    # Test document and questions
    test_data = {
        "documents": "https://hackrx.blob.core.windows.net/assets/policy.pdf?sv=2023-01-03&st=2025-07-04T09%3A11%3A24Z&se=2027-07-05T09%3A11%3A00Z&sr=b&sp=r&sig=N4a9OU0w0QXO6AOIBiu4bpl7AXvEZogeT%2FjUHNO7HzQ%3D",
        "questions": [
            "What is the grace period for premium payment?",
            "What is the waiting period for cataract surgery?",
            "What is the waiting period for pre-existing diseases?",
            "Does this policy cover maternity expenses?",
            "What is the No Claim Discount offered?"
        ]
    }
    
    # Test 1: Health check
    print("\nğŸ” Step 1: Testing health endpoint...")
    try:
        response = requests.get(f"{BASE_URL}/health", timeout=10)
        if response.status_code == 200:
            health_data = response.json()
            print("âœ… Health check passed")
            print(f"ğŸ“Š Status: {health_data.get('status')}")
            print(f"ğŸ¤– Models: {health_data.get('models', {})}")
        else:
            print(f"âŒ Health check failed: {response.status_code}")
            print(f"Response: {response.text}")
            return False
    except Exception as e:
        print(f"âŒ Health check error: {e}")
        print("ğŸ’¡ Make sure the server is running: python run_local_only.py")
        return False
    
    # Test 2: Root endpoint
    print("\nğŸ  Step 2: Testing root endpoint...")
    try:
        response = requests.get(f"{BASE_URL}/", timeout=10)
        if response.status_code == 200:
            root_data = response.json()
            print("âœ… Root endpoint working")
            print(f"ğŸ“ Message: {root_data.get('message')}")
            print(f"ğŸ¯ Features: {len(root_data.get('features', []))} listed")
        else:
            print(f"âš ï¸  Root endpoint returned: {response.status_code}")
    except Exception as e:
        print(f"âš ï¸  Root endpoint error: {e}")
    
    # Test 3: Main endpoint
    print("\nğŸ¯ Step 3: Testing main endpoint /api/v1/hackrx/run...")
    print(f"ğŸ“„ Document: {test_data['documents'][:60]}...")
    print(f"â“ Questions: {len(test_data['questions'])}")
    
    headers = {
        "Content-Type": "application/json"
    }
    
    try:
        print("ğŸš€ Sending request to local LLM server...")
        start_time = time.time()
        
        response = requests.post(
            f"{BASE_URL}/api/v1/hackrx/run",
            headers=headers,
            json=test_data,
            timeout=300  # 5 minute timeout for local processing
        )
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"ğŸ“¥ Response received in {total_time:.1f} seconds")
        print(f"ğŸ“Š Status Code: {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            answers = result.get("answers", [])
            
            print("ğŸ‰ SUCCESS: Local LLM server working perfectly!")
            print(f"ğŸ“ Received {len(answers)} answers")
            
            # Display answers
            print(f"\nğŸ“‹ ANSWERS FROM LOCAL LLM:")
            print("=" * 50)
            for i, (question, answer) in enumerate(zip(test_data["questions"], answers), 1):
                print(f"\nâ“ Q{i}: {question}")
                print(f"ğŸ’¬ A{i}: {answer}")
                print("-" * 50)
            
            # Performance summary
            print(f"\nâš¡ PERFORMANCE SUMMARY:")
            print("=" * 50)
            print(f"âœ… Total time: {total_time:.1f} seconds")
            print(f"âš¡ Average per question: {total_time/len(test_data['questions']):.1f} seconds")
            print(f"ğŸ’° Cost: $0.00 (completely free!)")
            print(f"ğŸ”„ Rate limits: None")
            print(f"ğŸ  Processing: 100% local")
            
            # Performance assessment
            if total_time <= 30:
                print("ğŸ¯ EXCELLENT: Under 30 seconds!")
            elif total_time <= 60:
                print("âœ… GOOD: Under 1 minute")
            elif total_time <= 120:
                print("âš ï¸  ACCEPTABLE: Under 2 minutes")
            else:
                print("ğŸŒ SLOW: Over 2 minutes (but still free!)")
            
            return True
            
        else:
            print(f"âŒ Request failed with status {response.status_code}")
            try:
                error_detail = response.json()
                print(f"Error details: {json.dumps(error_detail, indent=2)}")
            except:
                print(f"Raw error: {response.text}")
            return False
            
    except requests.exceptions.Timeout:
        print("âŒ Request timed out (over 5 minutes)")
        print("ğŸ’¡ Local LLM might be processing - this is normal for first request")
        return False
    except Exception as e:
        print(f"âŒ Request error: {e}")
        return False


def test_multiple_requests():
    """Test multiple requests to check consistency"""
    print(f"\nğŸ”„ TESTING MULTIPLE REQUESTS")
    print("=" * 50)
    
    BASE_URL = "http://localhost:8000"
    
    # Simple test data for quick testing
    test_data = {
        "documents": "https://hackrx.blob.core.windows.net/assets/policy.pdf?sv=2023-01-03&st=2025-07-04T09%3A11%3A24Z&se=2027-07-05T09%3A11%3A00Z&sr=b&sp=r&sig=N4a9OU0w0QXO6AOIBiu4bpl7AXvEZogeT%2FjUHNO7HzQ%3D",
        "questions": [
            "What is the grace period for premium payment?"
        ]
    }
    
    headers = {"Content-Type": "application/json"}
    
    print("ğŸ§ª Making 3 consecutive requests...")
    
    times = []
    for i in range(3):
        print(f"\nğŸ“¤ Request {i+1}/3...")
        
        try:
            start_time = time.time()
            response = requests.post(
                f"{BASE_URL}/api/v1/hackrx/run",
                headers=headers,
                json=test_data,
                timeout=120
            )
            end_time = time.time()
            
            request_time = end_time - start_time
            times.append(request_time)
            
            if response.status_code == 200:
                result = response.json()
                answer = result.get("answers", [""])[0]
                print(f"âœ… Request {i+1} successful ({request_time:.1f}s)")
                print(f"ğŸ“ Answer: {answer[:100]}...")
            else:
                print(f"âŒ Request {i+1} failed: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ Request {i+1} error: {e}")
    
    if times:
        avg_time = sum(times) / len(times)
        print(f"\nğŸ“Š Multiple Request Summary:")
        print(f"âœ… Successful requests: {len(times)}/3")
        print(f"â±ï¸  Times: {[f'{t:.1f}s' for t in times]}")
        print(f"âš¡ Average time: {avg_time:.1f}s")
        print(f"ğŸ¯ Consistency: {'Good' if max(times) - min(times) < 10 else 'Variable'}")


def main():
    """Main test function"""
    print("ğŸ  LOCAL-ONLY LLM SERVER TEST")
    print("=" * 60)
    print("Testing your local Ollama-based server")
    print("Endpoint: POST /api/v1/hackrx/run")
    print("=" * 60)
    
    # Test 1: Main functionality
    success = test_local_server()
    
    if success:
        # Test 2: Multiple requests
        test_multiple_requests()
        
        print(f"\nğŸ‰ LOCAL SERVER TEST COMPLETED!")
        print("=" * 60)
        print("âœ… Your local-only LLM server is working perfectly!")
        print("ğŸ¯ Benefits:")
        print("   â€¢ $0.00 cost per request")
        print("   â€¢ No API rate limits")
        print("   â€¢ Complete privacy (data stays local)")
        print("   â€¢ Works offline")
        print("   â€¢ Consistent performance")
        print("\nğŸš€ Your server is ready for production use!")
        print("ğŸ“¡ Server URL: http://localhost:8000")
        print("ğŸ¯ Endpoint: POST /api/v1/hackrx/run")
        
    else:
        print(f"\nâŒ LOCAL SERVER TEST FAILED")
        print("=" * 60)
        print("ğŸ”§ Troubleshooting steps:")
        print("1. Make sure Ollama is running: ollama serve")
        print("2. Check if models are downloaded:")
        print("   â€¢ ollama list")
        print("3. Start the server: python run_local_only.py")
        print("4. Check server logs for errors")
    
    print(f"\nğŸ•’ Test completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


if __name__ == "__main__":
    main()
